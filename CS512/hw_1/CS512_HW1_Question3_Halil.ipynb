{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["yHTn-62ArV3F"],"authorship_tag":"ABX9TyO2LLUhq/+IilX1FV+0tPaz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Reading the Data (including question 1 and question 2)"],"metadata":{"id":"YXZ9X-B_woQy"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n"],"metadata":{"id":"Ta9omtfsin7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"R5YxvE3ZYRdu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668944629374,"user_tz":-180,"elapsed":2628,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"2a9298e5-dca4-4514-c658-b21ae02cfdb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/Colab Notebooks/CS512/hw_1/train-features.txt\"\n","path_v2 = \"/content/drive/MyDrive/Colab Notebooks/CS512/hw_1/train-labels.txt\"\n","path_v3 = \"/content/drive/MyDrive/Colab Notebooks/CS512/hw_1/test-features.txt\"\n","path_v4 = \"/content/drive/MyDrive/Colab Notebooks/CS512/hw_1/test-labels.txt\""],"metadata":{"id":"-Ufw8wWvaV-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# There are no words or names in columns. So ıt would be better first name these features\n","\n","feature_words = []\n","for i in range(1,2501):\n","    feature_words.append(\"word{}\".format(i))"],"metadata":{"id":"Ik3OG4xNyEsT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# importing pandas\n","import pandas as pd\n","\n","# read text file into pandas DataFrame\n","traindf = pd.read_csv(path, sep=\" \", header = None, names = feature_words)\n","traindf_l = pd.read_csv(path_v2, sep=\" \", header = None, names=[\"label\"])\n","testdf = pd.read_csv(path_v3, sep=\" \", header = None, names = feature_words)\n","testdf_l = pd.read_csv(path_v4, sep=\" \", header = None, names=[\"label\"])\n","\n","# display DataFrame\n","traindf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"q9b6a7KGakIm","executionInfo":{"status":"ok","timestamp":1668944637359,"user_tz":-180,"elapsed":848,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"4c7f5731-ca5e-4fa1-beef-76eb1fa141fd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     word1  word2  word3  word4  word5  word6  word7  word8  word9  word10  \\\n","0        0      0      0      0      0      0      0      0      0       0   \n","1        0      0      0      0      0      0      0      0      0       0   \n","2        0      0      0      1      0      0      0      0      0       2   \n","3        0      0      0      0      0      0      0      0      0       0   \n","4        0      0      0      0      0      0      0      1      0       1   \n","..     ...    ...    ...    ...    ...    ...    ...    ...    ...     ...   \n","695      0      0      0      0      0      0      0      0      0       0   \n","696      0      0      1      0      0      0      3      0      0       0   \n","697      1      0      0      0      0      0      1      0      0       0   \n","698      1      0      0      0      0      0      1      0      0       0   \n","699      3      2     17      0      0      5     14      0      0       8   \n","\n","     ...  word2491  word2492  word2493  word2494  word2495  word2496  \\\n","0    ...         0         0         0         0         0         0   \n","1    ...         0         0         0         0         0         0   \n","2    ...         0         0         0         0         0         0   \n","3    ...         0         0         0         0         0         0   \n","4    ...         0         0         0         0         0         0   \n","..   ...       ...       ...       ...       ...       ...       ...   \n","695  ...         0         0         0         0         0         0   \n","696  ...         0         0         0         0         0         0   \n","697  ...         0         0         0         0         0         0   \n","698  ...         0         0         0         0         0         0   \n","699  ...         0         0         0         0         0         0   \n","\n","     word2497  word2498  word2499  word2500  \n","0           0         0         0         0  \n","1           0         0         0         0  \n","2           0         0         0         0  \n","3           0         0         0         0  \n","4           0         0         0         0  \n","..        ...       ...       ...       ...  \n","695         0         0         0         0  \n","696         0         0         0         0  \n","697         0         0         0         0  \n","698         0         0         0         0  \n","699         0         0         0         3  \n","\n","[700 rows x 2500 columns]"],"text/html":["\n","  <div id=\"df-e4fd5306-ff97-491c-b9fa-3d6f50e4f4fb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word1</th>\n","      <th>word2</th>\n","      <th>word3</th>\n","      <th>word4</th>\n","      <th>word5</th>\n","      <th>word6</th>\n","      <th>word7</th>\n","      <th>word8</th>\n","      <th>word9</th>\n","      <th>word10</th>\n","      <th>...</th>\n","      <th>word2491</th>\n","      <th>word2492</th>\n","      <th>word2493</th>\n","      <th>word2494</th>\n","      <th>word2495</th>\n","      <th>word2496</th>\n","      <th>word2497</th>\n","      <th>word2498</th>\n","      <th>word2499</th>\n","      <th>word2500</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>695</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>696</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>697</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>698</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>699</th>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>700 rows × 2500 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4fd5306-ff97-491c-b9fa-3d6f50e4f4fb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e4fd5306-ff97-491c-b9fa-3d6f50e4f4fb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e4fd5306-ff97-491c-b9fa-3d6f50e4f4fb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["# Question 1\n","# A balanced distribution we have in our labels, it is fifty-fifty\n","print(traindf_l['label'].value_counts().to_frame().T)\n","traindf_l['label'].value_counts().plot.bar()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"e7mh7cPRif4U","executionInfo":{"status":"ok","timestamp":1668944645457,"user_tz":-180,"elapsed":542,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"8b5aaf74-fc2b-425c-fcc6-45cf12c69a72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["         0    1\n","label  350  350\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fe60b75cc90>"]},"metadata":{},"execution_count":127},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOE0lEQVR4nO3cb4ilZ3nH8e/PbPxDlSZppsu6f7pBVyQW3Mh0TbEvbII1SV9shDYkL3QJgbElAQUpRl9UhQYUqgHBBlaSuhZrXPxDFk1t0zVFpJg4seuaTUydatLdYc2OGqNBmjbr1RdzB08mM3tm5syZce98P3A4z3Pd93Oe68DhN4d7nuekqpAk9eVFG92AJGntGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR3atNENAFx44YW1c+fOjW5Dks4qDzzwwI+ramKxsd+IcN+5cyfT09Mb3YYknVWSPLbUmMsyktQhw12SOmS4S1KHDHdJ6pDhLkkdGhruSV6a5P4k30lyLMmHWv1TSX6Y5Eh77G71JPl4kpkkR5O8YdxvQpL0XMu5FPJp4LKqeirJucA3kvxTG/urqvr8gvlXArva443Abe1ZkrROhn5zr3lPtd1z2+NMPwK/F/h0O+6bwHlJtozeqiRpuZZ1E1OSc4AHgFcDn6iq+5L8JXBLkr8GDgM3V9XTwFbg+MDhJ1rt5ILXnAKmAHbs2DHq+1gXO2/+yka30JVHP/ynG91CN/xsrq0ePpvL+odqVZ2uqt3ANmBPkt8H3ge8FvgD4ALgvSs5cVXtr6rJqpqcmFj07llJ0iqt6GqZqvoZcC9wRVWdbEsvTwN/D+xp02aB7QOHbWs1SdI6Wc7VMhNJzmvbLwPeAnzv2XX0JAGuBh5shxwC3tGumrkUeLKqTi7y0pKkMVnOmvsW4EBbd38RcLCqvpzka0kmgABHgL9o8+8GrgJmgF8C169925KkMxka7lV1FLhkkfplS8wv4MbRW5MkrZZ3qEpShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUNDwz3JS5Pcn+Q7SY4l+VCrX5TkviQzST6X5MWt/pK2P9PGd473LUiSFlrON/engcuq6vXAbuCKJJcCHwFurapXA08AN7T5NwBPtPqtbZ4kaR0NDfea91TbPbc9CrgM+HyrHwCubtt72z5t/PIkWbOOJUlDLWvNPck5SY4Ap4B7gP8CflZVz7QpJ4CtbXsrcBygjT8J/M5aNi1JOrNlhXtVna6q3cA2YA/w2lFPnGQqyXSS6bm5uVFfTpI0YEVXy1TVz4B7gT8EzkuyqQ1tA2bb9iywHaCN/zbwk0Vea39VTVbV5MTExCrblyQtZjlXy0wkOa9tvwx4C/Aw8yH/Z23aPuCutn2o7dPGv1ZVtZZNS5LObNPwKWwBDiQ5h/k/Bger6stJHgLuTPI3wH8At7f5twP/kGQG+Clw7Rj6liSdwdBwr6qjwCWL1H/A/Pr7wvr/AH++Jt1JklbFO1QlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOjQ03JNsT3JvkoeSHEvyrlb/YJLZJEfa46qBY96XZCbJI0neOs43IEl6vk3LmPMM8J6q+naSVwAPJLmnjd1aVX87ODnJxcC1wOuAVwL/muQ1VXV6LRuXJC1t6Df3qjpZVd9u278AHga2nuGQvcCdVfV0Vf0QmAH2rEWzkqTlWdGae5KdwCXAfa10U5KjSe5Icn6rbQWODxx2gjP/MZAkrbFlh3uSlwNfAN5dVT8HbgNeBewGTgIfXcmJk0wlmU4yPTc3t5JDJUlDLCvck5zLfLB/pqq+CFBVj1fV6ar6FfBJfr30MgtsHzh8W6s9R1Xtr6rJqpqcmJgY5T1IkhZYztUyAW4HHq6qjw3UtwxMexvwYNs+BFyb5CVJLgJ2AfevXcuSpGGWc7XMm4C3A99NcqTV3g9cl2Q3UMCjwDsBqupYkoPAQ8xfaXOjV8pI0voaGu5V9Q0giwzdfYZjbgFuGaEvSdIIvENVkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdGhruSbYnuTfJQ0mOJXlXq1+Q5J4k32/P57d6knw8yUySo0neMO43IUl6ruV8c38GeE9VXQxcCtyY5GLgZuBwVe0CDrd9gCuBXe0xBdy25l1Lks5oaLhX1cmq+nbb/gXwMLAV2AscaNMOAFe37b3Ap2veN4HzkmxZ884lSUta0Zp7kp3AJcB9wOaqOtmGfgRsbttbgeMDh51oNUnSOll2uCd5OfAF4N1V9fPBsaoqoFZy4iRTSaaTTM/Nza3kUEnSEMsK9yTnMh/sn6mqL7by488ut7TnU60+C2wfOHxbqz1HVe2vqsmqmpyYmFht/5KkRSznapkAtwMPV9XHBoYOAfva9j7groH6O9pVM5cCTw4s30iS1sGmZcx5E/B24LtJjrTa+4EPAweT3AA8BlzTxu4GrgJmgF8C169px5KkoYaGe1V9A8gSw5cvMr+AG0fsS5I0Au9QlaQOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SerQ0HBPckeSU0keHKh9MMlskiPtcdXA2PuSzCR5JMlbx9W4JGlpy/nm/ingikXqt1bV7va4GyDJxcC1wOvaMX+X5Jy1alaStDxDw72qvg78dJmvtxe4s6qerqofAjPAnhH6kyStwihr7jclOdqWbc5vta3A8YE5J1pNkrSOVhvutwGvAnYDJ4GPrvQFkkwlmU4yPTc3t8o2JEmLWVW4V9XjVXW6qn4FfJJfL73MAtsHpm5rtcVeY39VTVbV5MTExGrakCQtYVXhnmTLwO7bgGevpDkEXJvkJUkuAnYB94/WoiRppTYNm5Dks8CbgQuTnAA+ALw5yW6ggEeBdwJU1bEkB4GHgGeAG6vq9HhalyQtZWi4V9V1i5RvP8P8W4BbRmlKkjQa71CVpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHhoZ7kjuSnEry4EDtgiT3JPl+ez6/1ZPk40lmkhxN8oZxNi9JWtxyvrl/CrhiQe1m4HBV7QIOt32AK4Fd7TEF3LY2bUqSVmJouFfV14GfLijvBQ607QPA1QP1T9e8bwLnJdmyVs1KkpZntWvum6vqZNv+EbC5bW8Fjg/MO9FqkqR1NPI/VKuqgFrpcUmmkkwnmZ6bmxu1DUnSgNWG++PPLre051OtPgtsH5i3rdWep6r2V9VkVU1OTEyssg1J0mJWG+6HgH1tex9w10D9He2qmUuBJweWbyRJ62TTsAlJPgu8GbgwyQngA8CHgYNJbgAeA65p0+8GrgJmgF8C14+hZ0nSEEPDvaquW2Lo8kXmFnDjqE1JkkbjHaqS1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHdo0ysFJHgV+AZwGnqmqySQXAJ8DdgKPAtdU1ROjtSlJWom1+Ob+x1W1u6om2/7NwOGq2gUcbvuSpHU0jmWZvcCBtn0AuHoM55AkncGo4V7AvyR5IMlUq22uqpNt+0fA5hHPIUlaoZHW3IE/qqrZJL8L3JPke4ODVVVJarED2x+DKYAdO3aM2IYkadBI39yrarY9nwK+BOwBHk+yBaA9n1ri2P1VNVlVkxMTE6O0IUlaYNXhnuS3krzi2W3gT4AHgUPAvjZtH3DXqE1KklZmlGWZzcCXkjz7Ov9YVV9N8i3gYJIbgMeAa0ZvU5K0EqsO96r6AfD6Reo/AS4fpSlJ0mi8Q1WSOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ2ML9yRXJHkkyUySm8d1HknS840l3JOcA3wCuBK4GLguycXjOJck6fnG9c19DzBTVT+oqv8F7gT2julckqQFNo3pdbcCxwf2TwBvHJyQZAqYartPJXlkTL28EF0I/HijmxgmH9noDrQB/Gyurd9bamBc4T5UVe0H9m/U+XuWZLqqJje6D2khP5vrZ1zLMrPA9oH9ba0mSVoH4wr3bwG7klyU5MXAtcChMZ1LkrTAWJZlquqZJDcB/wycA9xRVcfGcS4tyuUu/abys7lOUlUb3YMkaY15h6okdchwl6QOGe6S1KENu85dayfJa5m/A3hrK80Ch6rq4Y3rStJG8pv7WS7Je5n/eYcA97dHgM/6g236TZXk+o3uoXdeLXOWS/KfwOuq6v8W1F8MHKuqXRvTmbS0JP9dVTs2uo+euSxz9vsV8ErgsQX1LW1M2hBJji41BGxez15eiAz3s9+7gcNJvs+vf6xtB/Bq4KYN60qaD/C3Ak8sqAf49/Vv54XFcD/LVdVXk7yG+Z9ZHvyH6req6vTGdSbxZeDlVXVk4UCSf1v/dl5YXHOXpA55tYwkdchwl6QOGe6S1CHDXZI6ZLhLUof+H0GDeKgmzxT0AAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["#Question 2\n","# There we have 2500 variables (words) in the dataset. First, we should calculate priors and that makes 1 parameter\n","# We should calculate P(Xi | Y = Spam) and P(Xi | Y = Nonspam) for every one of them which makes\n","# 2500 + 2500 = 5000 parameters. This means that we have 5001 parameters in total. Second prior no need to be calculated as we can easly find it by substracting it from 1."],"metadata":{"id":"rUZRf7bEzCFl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 3 (In this part, I have dealt with Question 3.3 and 3.4)\n"],"metadata":{"id":"5znOe2q70tFp"}},{"cell_type":"markdown","source":["## Dictionary for word frequencies\n"],"metadata":{"id":"yHTn-62ArV3F"}},{"cell_type":"code","source":["#Which word is more frequent\n","worddict = [] #unique words\n","wordlist = [] #corresponding frequencies of each word\n","\n","for words in traindf.columns:\n","  worddict.append(words)\n","  totalwords = traindf[words].sum()\n","  wordlist.append(totalwords)\n","\n","dicti = {} #dict where keys are worddict and values are wordlist\n","for key in worddict:\n","    for value in wordlist:\n","        dicti[key] = value\n","        wordlist.remove(value)\n","        break"],"metadata":{"id":"w4rpoHzKYUdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(worddict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9YxMSGbUn93","executionInfo":{"status":"ok","timestamp":1668784767229,"user_tz":-180,"elapsed":6,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"33fc5bbc-b5c2-46fd-f9e3-fee99622d707"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2500"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["#Vocabulary\n","worddict = np.array(worddict)\n","print(len(worddict))\n","\n","#Wordcounts\n","word_counts = np.array(wordlist)\n","print(len(word_counts))"],"metadata":{"id":"cV6YWPP91UAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Which word is more frequent\n","worddict = [] #unique words\n","wordlist = [] #corresponding frequencies of each word\n","\n","for words in traindf.columns:\n","  worddict.append(words)\n","  totalwords = traindf[words].sum()\n","  wordlist.append(totalwords)\n"],"metadata":{"id":"ocNpNk0DRJFl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Naive Bayes"],"metadata":{"id":"zG8PzpWQrc8e"}},{"cell_type":"code","source":["#Constructing Naive Bayes through finding parameters step by step\n","priors = []\n","def find_priors(data,target):\n","  categories = sorted(list(data[target].unique()))\n","  for value in categories:\n","    priors.append(len(data[data[target]==value])/len(data))\n","  return priors\n","\n","find_priors(traindf_l, 'label')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25NRFLv9Dye5","executionInfo":{"status":"ok","timestamp":1668771002760,"user_tz":-180,"elapsed":284,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"a64e76e7-a0a3-46aa-fdb5-7bf4084c383f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5, 0.5]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# First, I will merge traindf and traindf_l datasets because dealing only with feature vectors confuses me.\n","traindata = pd.concat([traindf, traindf_l], axis=1)\n","testdata = pd.concat([testdf, testdf_l], axis = 1)\n","traindata.tail()"],"metadata":{"id":"zw1xwcsuj6nd","executionInfo":{"status":"ok","timestamp":1668944716849,"user_tz":-180,"elapsed":364,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"bc24eed0-3445-4e07-ad91-7d3549f54441"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     word1  word2  word3  word4  word5  word6  word7  word8  word9  word10  \\\n","695      0      0      0      0      0      0      0      0      0       0   \n","696      0      0      1      0      0      0      3      0      0       0   \n","697      1      0      0      0      0      0      1      0      0       0   \n","698      1      0      0      0      0      0      1      0      0       0   \n","699      3      2     17      0      0      5     14      0      0       8   \n","\n","     ...  word2492  word2493  word2494  word2495  word2496  word2497  \\\n","695  ...         0         0         0         0         0         0   \n","696  ...         0         0         0         0         0         0   \n","697  ...         0         0         0         0         0         0   \n","698  ...         0         0         0         0         0         0   \n","699  ...         0         0         0         0         0         0   \n","\n","     word2498  word2499  word2500  label  \n","695         0         0         0      1  \n","696         0         0         0      1  \n","697         0         0         0      1  \n","698         0         0         0      1  \n","699         0         0         3      1  \n","\n","[5 rows x 2501 columns]"],"text/html":["\n","  <div id=\"df-dca11188-e456-48c0-b861-d4bf128cf1d7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word1</th>\n","      <th>word2</th>\n","      <th>word3</th>\n","      <th>word4</th>\n","      <th>word5</th>\n","      <th>word6</th>\n","      <th>word7</th>\n","      <th>word8</th>\n","      <th>word9</th>\n","      <th>word10</th>\n","      <th>...</th>\n","      <th>word2492</th>\n","      <th>word2493</th>\n","      <th>word2494</th>\n","      <th>word2495</th>\n","      <th>word2496</th>\n","      <th>word2497</th>\n","      <th>word2498</th>\n","      <th>word2499</th>\n","      <th>word2500</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>695</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>696</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>697</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>698</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>699</th>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 2501 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dca11188-e456-48c0-b861-d4bf128cf1d7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dca11188-e456-48c0-b861-d4bf128cf1d7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dca11188-e456-48c0-b861-d4bf128cf1d7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":129}]},{"cell_type":"code","source":["# Seperating spam and nonspam emails. I am doing this so that I can calcualte number of spam and non spam emails\n","spam_email = traindata[traindata['label'] == 1] # This filters spam emails\n","nonspam_email = traindata[traindata['label'] == 0] # This filters nonspam emails\n","\n","# Number of nonspams\n","words_in_nonspam = nonspam_email.sum(axis=1) # This stores the total number of words mentioned in nonspam emails\n","nonspam_total = words_in_nonspam.sum()\n","\n","# Number of spams\n","words_in_spam = spam_email.sum(axis=1)\n","spam_total = words_in_spam.sum()\n","\n","# Parameters\n","spam_parameters = {word:0 for word in feature_words} # An empty dictionary with comprehension where key is feature/word and value will be usege ratio per feature/word\n","nonspam_parameters = {word:0 for word in feature_words}\n","\n","# Going over paramters and finalize them. Beforehand, I calcucalted priors and saved them into lists in above line but this is better I guess\n","for i in feature_words:\n","  spam_parameters[i] = (spam_email[i].sum() / spam_total) # this will give me the frequency ratio per feature/word in spam emails\n","  nonspam_parameters[i] = (nonspam_email[i].sum() / nonspam_total)\n","prior_spam = len(traindata.query(\"label == 1\"))/len(traindata)\n","prior_nonspam = 1- prior_spam\n","\n"],"metadata":{"id":"cQosby3j1ZHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First classifier without alpha laplace smoothing. This function is supposed to take test dataframe and calculate conditional probabilities for entire training data.\n","import math\n","\n","def nb_classifier(test_data_features):\n","  given_email_pspam = []\n","  given_email_pnonspam = []\n","  for column_index in range(0, len(test_data_features)):\n","    k = 0\n","    z = 0\n","    for i in feature_words:\n","      k = k + ([math.log(spam_parameters[i]) if spam_parameters[i] > 0 else 0][0] * test_data_features.iloc[column_index][i]) #By following instructions, I take the log of these probs as multipyling might cause underflow issue.\n","      z = z + ([math.log(nonspam_parameters[i]) if nonspam_parameters[i] > 0 else 0][0] * test_data_features.iloc[column_index][i])\n","    given_email_pspam.append(math.log(prior_spam) + k) #Now, I will sum priors and conditionals for spam\n","    given_email_pnonspam.append(math.log(prior_nonspam) + z) #Now, I will sum priors and conditionals for non-spam\n","  return given_email_pspam, given_email_pnonspam\n","\n","given_email_pspam, given_email_pnonspam = nb_classifier(testdf)\n","testdata['probabil_of_nonspam'] = given_email_pnonspam # These two will add two new columns representing posterior probabilities\n","testdata['probabil_of_spam'] = given_email_pspam"],"metadata":{"id":"PXC0c_29CKI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#this function create a new column called \"predicted\" by a condition of whether Prob of spam or nonspam is bigger. It will compute the maximum arg via MLE\n","testdata['predicted'] = np.where(testdata['probabil_of_spam'] > testdata['probabil_of_nonspam'], 1, 0)  # Incase of ties,you should predict “non-spam”.\n","testdata.head()"],"metadata":{"id":"c62fGE0hkMzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["''' Accuracy metric. The score is low. MLE estimate is a bad idea in this situation because it only takes into account\n","the likelihood regarding what is the hypothesis (class:spam or non spam) that maximizes the likelihood of P(d|h).\n","The thing is that MLE does not consider the prior knowledge and hence it does not update probabilities in accordence with this. Plus there are lots of zeros as well in my training data.\n","MLE is not that much goog in handling with this over zero issue.\n"," '''\n","\n","acc = np.sum(np.equal(testdata['label'], testdata['predicted'])) / len(testdata['label'])\n","print(f\"Accuracy score with MLE is {acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdTrLo2slbUH","executionInfo":{"status":"ok","timestamp":1668947520228,"user_tz":-180,"elapsed":464,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"11c3a6e7-8246-4cfc-f59c-17edee0d55dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score with MLE is 0.596\n"]}]},{"cell_type":"markdown","source":["## Question 3.4 (MAP estimate of θ parameters using a fair Dirichlet prior.)"],"metadata":{"id":"QOIO8T-2mvF8"}},{"cell_type":"code","source":["alpha = 1\n","\n","# Seperating spam and nonspam emails. I am doing this filter for the reason I mentioned above: P(Xi | Y = Spam) and P(Xi | Y = Nonspam)\n","s_email = traindata[traindata['label'] == 1]\n","nos_email = traindata[traindata['label'] == 0]\n","\n","# Parameters for MAP\n","spam_parameters_v2 = {word:0 for word in feature_words}\n","nonspam_parameters_v2 = {word:0 for word in feature_words}\n","\n","# Going over parameters and finalize them. Differently from previous one, this case will add alpha smoothing.\n","for i in feature_words:\n","  spam_parameters_v2[i] = (s_email[i].sum() + alpha) / (spam_total + (alpha * 2500)) # this will give me the frequency ratio per feature/word in spam emails\n","  nonspam_parameters_v2[i] = (nos_email[i].sum() + alpha) / (nonspam_total + (alpha * 2500)) #Multiply aplha with 2500 as I have that much feature words in my data\n","prior_spam = len(traindata.query(\"label == 1\"))/len(traindata)\n","prior_nonspam = 1- prior_spam\n","\n","#Classifier for MAP. THe only difference is alpha which is supposed to make predicitons a lot different than MLE previous case\n","\n","def nb_classifier_v2(test_data_features):\n","  given_email_pspam_v2 = []\n","  given_email_pnonspam_v2 = []\n","  for column_index in range(0, len(test_data_features)):\n","    k = 0\n","    z = 0\n","    for i in feature_words:\n","      k = k + ([math.log(spam_parameters_v2[i]) if spam_parameters_v2[i] > 0 else 0][0] * test_data_features.iloc[column_index][i])\n","      z = z + ([math.log(nonspam_parameters_v2[i]) if nonspam_parameters_v2[i] > 0 else 0][0] * test_data_features.iloc[column_index][i])\n","    given_email_pspam_v2.append(math.log(prior_spam) + k)\n","    given_email_pnonspam_v2.append(math.log(prior_nonspam) + z)\n","  return given_email_pspam_v2, given_email_pnonspam_v2\n","\n","given_email_pspam_v2, given_email_pnonspam_v2 = nb_classifier_v2(testdf)\n","testdata['probabil_of_nonspam'] = given_email_pnonspam_v2 # These two will add two new columns representing posterior probabilities\n","testdata['probabil_of_spam'] = given_email_pspam_v2"],"metadata":{"id":"-gRZlRRxmyS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#this function create a new column called \"predicted\" by a condition of whether Prob of spam or nonspam is bigger. It will compute the maximum arg via MAP\n","testdata['predicted'] = np.where(testdata['probabil_of_spam'] > testdata['probabil_of_nonspam'], 1, 0) # Incase of ties,you should predict “non-spam”.\n","testdata.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"qWc1S1q6qTrZ","executionInfo":{"status":"ok","timestamp":1668947703434,"user_tz":-180,"elapsed":416,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"2802e38a-ebfd-4cf3-a453-d413473fb37f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   word1  word2  word3  word4  word5  word6  word7  word8  word9  word10  ...  \\\n","0      0      0      0      0      0      0      0      1      1       0  ...   \n","1      1      1      0      0      0      0      0      0      1       0  ...   \n","2      0      0      0      2      0      0      0      4      0       0  ...   \n","3      0      0      0      1      0      0      2      0      0       0  ...   \n","4      0      0      1      0      0      0      0      1      0       0  ...   \n","\n","   word2495  word2496  word2497  word2498  word2499  word2500  label  \\\n","0         0         0         0         0         6         0      0   \n","1         0         0         0         0        10         0      0   \n","2         0         0         0         0         6         0      0   \n","3         0         0         0         0         9         0      0   \n","4         0         0         0         0        10         0      0   \n","\n","   probabil_of_nonspam  probabil_of_spam  predicted  \n","0          -463.571783       -542.454395          0  \n","1          -310.256421       -351.687145          0  \n","2          -681.705604       -916.895369          0  \n","3          -720.315298       -794.514449          0  \n","4          -312.076719       -346.151278          0  \n","\n","[5 rows x 2504 columns]"],"text/html":["\n","  <div id=\"df-5790b2e7-b13a-4303-ab74-06c1ba77e155\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word1</th>\n","      <th>word2</th>\n","      <th>word3</th>\n","      <th>word4</th>\n","      <th>word5</th>\n","      <th>word6</th>\n","      <th>word7</th>\n","      <th>word8</th>\n","      <th>word9</th>\n","      <th>word10</th>\n","      <th>...</th>\n","      <th>word2495</th>\n","      <th>word2496</th>\n","      <th>word2497</th>\n","      <th>word2498</th>\n","      <th>word2499</th>\n","      <th>word2500</th>\n","      <th>label</th>\n","      <th>probabil_of_nonspam</th>\n","      <th>probabil_of_spam</th>\n","      <th>predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-463.571783</td>\n","      <td>-542.454395</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-310.256421</td>\n","      <td>-351.687145</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-681.705604</td>\n","      <td>-916.895369</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-720.315298</td>\n","      <td>-794.514449</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-312.076719</td>\n","      <td>-346.151278</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 2504 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5790b2e7-b13a-4303-ab74-06c1ba77e155')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5790b2e7-b13a-4303-ab74-06c1ba77e155 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5790b2e7-b13a-4303-ab74-06c1ba77e155');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":152}]},{"cell_type":"code","source":["# The score is exatcly same with sklearn as MAP does a better job in this task by considering priors.\n","acc_2 = np.sum(np.equal(testdata['label'], testdata['predicted'])) / len(testdata['label'])\n","print(f\"Accuracy score with MAP is {acc_2:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh-91czFum9E","executionInfo":{"status":"ok","timestamp":1668947705316,"user_tz":-180,"elapsed":457,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"663b215a-9dba-406f-c6f8-b48614903b57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score with MAP is 0.973\n"]}]},{"cell_type":"markdown","source":["## Sklearn Comparison (The result is almost same)"],"metadata":{"id":"kDQxsIvAu2nn"}},{"cell_type":"code","source":["train_split = pd.read_csv(path, sep=\" \", header = None, names = feature_columns)\n","train_label = pd.read_csv(path_v2, sep=\" \", header = None, names=[\"label\"])\n","test_split = pd.read_csv(path_v3, sep=\" \", header = None, names = feature_columns)\n","test_label = pd.read_csv(path_v4, sep=\" \", header = None, names=[\"label\"])"],"metadata":{"id":"eHviBT3IvGBT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_split = train_split.to_numpy()\n","train_label = train_label.to_numpy()\n","test_split = test_split.to_numpy()\n","test_label = test_label.to_numpy()"],"metadata":{"id":"j-1eJLjVvGYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB()\n","clf.fit(train_split, train_label)\n","clf.predict(test_split)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MU2H3E3Yu6f-","executionInfo":{"status":"ok","timestamp":1668942806914,"user_tz":-180,"elapsed":348,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"f2bd8963-64c8-4f94-937e-c28488f8e05e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["print(\"Test accuracy with SKlearn:\",clf.score(test_split,test_label))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSTndpy7wEVW","executionInfo":{"status":"ok","timestamp":1668943065747,"user_tz":-180,"elapsed":411,"user":{"displayName":"Halil İbrahim Ergül","userId":"12340829045287559998"}},"outputId":"cd70876a-554e-411a-c1bf-ad56005466cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy with SKlearn: 0.9730769230769231\n"]}]}]}